{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression adalah salah satu jenis algoritma pembelajaran mesin yang sangat mendasar, tetapi sangat cukup praktis dalam aplikasinya. Di sini, kita akan belajar mengenai aplikasi Logistic Regression dengan menggunakan data berupa Breast Cancer Dataset. Kita diminta mengklasifikasikan apakah suatu jenis kanker tergolong Benignant (Jinak) ataukah Malignant (Ganas). Karena ini bersifat dasar, kita belum perlu melakukan preprocessing data. Dalam kenyataannya, kita perlu melakukan preprocessing data karena data yang ada secara nyata biasanya tidak lengkap ataupun mengandung anomali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn import datasets\n",
    "np.set_printoptions(precision=3, suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = datasets.load_breast_cancer(as_frame=True)\n",
    "\n",
    "df = breast_cancer.data\n",
    "df[\"is_malignant\"] = 0\n",
    "df.loc[breast_cancer.target ==1, \"is_malignant\"] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, test_sixe=0.2):\n",
    "    data_size = len(X)\n",
    "    test_size = int(data_size * test_size)\n",
    "    test_index = random.sample(range(data_size), test_size)\n",
    "    train_index = [i for i in range(data_size) if i not in test_index]\n",
    "    X_train = X[train_index]\n",
    "    Y_train = Y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    Y_test = Y[test_index]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression : Linear Regression dengan fungsi sigmoid\n",
    "\n",
    "Misal regresi linear yang digunakan untuk memprediksi suatu nilai variabel dependen memiliki rumus $$ z = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + ... + \\theta_{n} x_n  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maka rumus regresi logistik yang digunakan untuk mengategorikan secara biner dua variabel adalah\n",
    "\n",
    "$$ Ln(\\frac{P}{1-P}) = \\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + ... + \\theta_{n} x_n  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "atau dapat dituliskan $$ P =  \\frac{e^{\\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + ... + \\theta_{n} x_n}}{1+e^{\\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + ... + \\theta_{n} x_n}} $$ atau $$P =  \\frac{1}{1+e^{-(\\theta_{0} + \\theta_{1} x_1 + \\theta_{2} x_2 + ... + \\theta_{n} x_n)}} $$\n",
    "$$P =  \\frac{1}{1+e^{-z}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catatan : Pada beberapa literatur dan beberapa bagian selanjutnya, P akan ditulis sebagai $h(z)$ atau sebuah fungsi yang memprediksi suatu nilai berdasarkan masukan nilai z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mengklasifikasikan 2 kelas, kita bisa tuliskan $$ P = \\{^{\\ge0.5,class=1 }_{<0.5,class=0}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena z berada di rentang $-\\infty$ sampai $\\infty$, bisa simpulkan bahwa P akan berada di rentang 0 sampai 1. Bila z=0, P akan bernilai 0,5 yang menandakan batas klasifikasi 2 kelas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menentukan Persamaan Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperti pada kasus regresi linear, kita bisa menyelesaikan permasalahan regresi logistik dengan menggunakan metode yang serupa. Kita bisa mencari nilai secara non iteratif dengan menyelesaikan persamaan Least Square berupa $X\\hat{\\theta}=Y$ yang merupakan permasalahan mendasar Aljabar Linear atau kita bisa menyelesaikan secara iteratif dengan menggunakan Metode Gradient Descent yang disimbolkan dengan $$ \\theta_j = \\theta_j - \\alpha \\nabla $$\n",
    "\n",
    "Dengan $\\theta$ sebagai koefisien persamaan yang kita cari, $\\alpha$ sebagai learning rate, dan $\\nabla$ sebagai gradien dari Cost Function, atau secara matematis \n",
    "$$ \\nabla = \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "Secara umum, cost function suatu persamaan Logistic Regression bisa dinyatakan sebagai penjumlahan Loss Function untuk setiap data-data yang dijadikan sampel\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(x(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(x(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ adalah jumlah data yang diberikan\n",
    "* $y^{(i)}$ adalah nilai yang sebenarnya\n",
    "* $h(x^{(i)})$ adalah nilai yang diprediksi oleh persamaan Logistic Regression\n",
    "\n",
    "Untuk satu data, Loss function dapat dinyatakan sebagai \n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(x(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(x(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* Karena nilai h berada pada kisaran $0\\le h \\le 1$, $log(h)$ pasti negatif. Itulah yang menyebabkan adanya pengali -1 di depan\n",
    "* Semisal nilai prediksi persamaan adalah 1 ($h(x(\\theta)) = 1$) dan label dari data 'y' juga 1, nilai loss suatu fungsi adalah 0. \n",
    "* Begitu pula, semisal nilai prediksi persamaan adalah 0 ($h(x(\\theta)) = 0$) dan label dari data 'y' juga 1, nilai loss suatu fungsi adalah 0. \n",
    "* Dapat dilihat pula, bila nilai prediksi $h(x(\\theta))=0$ dan label data 'y' bernilai 1 ataupun sebaliknya, lossnya akan bernilai tak hingga sehingga Cost Function akan menjadi besar saat ada kesalahan prediksi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secara keseluruhan, nilai gradien dari Cost Function ini adalah\n",
    "$$ \\nabla = \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(x(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(x(\\theta)^{(i)}))\\tag{5} =\n",
    "-\\frac{1}{m} \\sum_{i=1}^m (h(x(\\theta)^{(i)})-y^{(i)})x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehingga persamaan tersebut bisa kita gunakan untuk menyelesaikan permasalahan Logistic Regression ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The number of iterations 'num_iters\" is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are 'm' training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(x,y,theta,alpha,iterations):\n",
    "    \"\"\"\n",
    "    x: input data\n",
    "    y: output data\n",
    "    theta: initial theta\n",
    "    alpha: learning rate\n",
    "    iterations: number of iterations\n",
    "    J: cost function\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    J_history = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        z = np.dot(x,theta)\n",
    "        h = sigmoid(z)\n",
    "\n",
    "        J_history[i] = (-1/m) * (np.dot(y.T,np.log(h)) + np.dot((1-y).T,np.log(1-h)))\n",
    "        print(\"Iteration: \",i,\" Cost: %.4f\"%float(J_history[i]))\n",
    "        grad = (1/m) * np.dot(x.T,(h-y))\n",
    "        theta = theta - alpha * grad\n",
    "    print(\"Final cost: %.4f\"%float(J_history[-1]))\n",
    "    visualize_cost(J_history)\n",
    "    return theta\n",
    "\n",
    "def visualize_cost(J_history):\n",
    "    x = np.arange(len(J_history))\n",
    "    plt.plot(x,J_history)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"is_malignant\"],axis=1).values\n",
    "Y = df[\"is_malignant\"].values\n",
    "X_train, Y_train, X_test, Y_test = split_data(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0  Cost: 0.6931\n",
      "Iteration:  1  Cost: 0.7439\n",
      "Iteration:  2  Cost: 0.9891\n",
      "Iteration:  3  Cost: 1.5071\n",
      "Iteration:  4  Cost: 0.9495\n",
      "Iteration:  5  Cost: 1.4742\n",
      "Iteration:  6  Cost: 0.9365\n",
      "Iteration:  7  Cost: 1.4617\n",
      "Iteration:  8  Cost: 0.9109\n",
      "Iteration:  9  Cost: 1.4345\n",
      "Iteration:  10  Cost: 0.8954\n",
      "Iteration:  11  Cost: 1.4144\n",
      "Iteration:  12  Cost: 0.8763\n",
      "Iteration:  13  Cost: 1.3873\n",
      "Iteration:  14  Cost: 0.8624\n",
      "Iteration:  15  Cost: 1.3632\n",
      "Iteration:  16  Cost: 0.8476\n",
      "Iteration:  17  Cost: 1.3356\n",
      "Iteration:  18  Cost: 0.8357\n",
      "Iteration:  19  Cost: 1.3099\n",
      "Iteration:  20  Cost: 0.8234\n",
      "Iteration:  21  Cost: 1.2825\n",
      "Iteration:  22  Cost: 0.8128\n",
      "Iteration:  23  Cost: 1.2567\n",
      "Iteration:  24  Cost: 0.8021\n",
      "Iteration:  25  Cost: 1.2302\n",
      "Iteration:  26  Cost: 0.7924\n",
      "Iteration:  27  Cost: 1.2047\n",
      "Iteration:  28  Cost: 0.7826\n",
      "Iteration:  29  Cost: 1.1791\n",
      "Iteration:  30  Cost: 0.7734\n",
      "Iteration:  31  Cost: 1.1542\n",
      "Iteration:  32  Cost: 0.7643\n",
      "Iteration:  33  Cost: 1.1294\n",
      "Iteration:  34  Cost: 0.7555\n",
      "Iteration:  35  Cost: 1.1051\n",
      "Iteration:  36  Cost: 0.7469\n",
      "Iteration:  37  Cost: 1.0810\n",
      "Iteration:  38  Cost: 0.7384\n",
      "Iteration:  39  Cost: 1.0573\n",
      "Iteration:  40  Cost: 0.7301\n",
      "Iteration:  41  Cost: 1.0339\n",
      "Iteration:  42  Cost: 0.7218\n",
      "Iteration:  43  Cost: 1.0109\n",
      "Iteration:  44  Cost: 0.7137\n",
      "Iteration:  45  Cost: 0.9881\n",
      "Iteration:  46  Cost: 0.7056\n",
      "Iteration:  47  Cost: 0.9658\n",
      "Iteration:  48  Cost: 0.6975\n",
      "Iteration:  49  Cost: 0.9438\n",
      "Iteration:  50  Cost: 0.6895\n",
      "Iteration:  51  Cost: 0.9222\n",
      "Iteration:  52  Cost: 0.6815\n",
      "Iteration:  53  Cost: 0.9009\n",
      "Iteration:  54  Cost: 0.6734\n",
      "Iteration:  55  Cost: 0.8800\n",
      "Iteration:  56  Cost: 0.6654\n",
      "Iteration:  57  Cost: 0.8595\n",
      "Iteration:  58  Cost: 0.6573\n",
      "Iteration:  59  Cost: 0.8393\n",
      "Iteration:  60  Cost: 0.6492\n",
      "Iteration:  61  Cost: 0.8194\n",
      "Iteration:  62  Cost: 0.6411\n",
      "Iteration:  63  Cost: 0.8000\n",
      "Iteration:  64  Cost: 0.6329\n",
      "Iteration:  65  Cost: 0.7808\n",
      "Iteration:  66  Cost: 0.6247\n",
      "Iteration:  67  Cost: 0.7620\n",
      "Iteration:  68  Cost: 0.6164\n",
      "Iteration:  69  Cost: 0.7435\n",
      "Iteration:  70  Cost: 0.6080\n",
      "Iteration:  71  Cost: 0.7253\n",
      "Iteration:  72  Cost: 0.5997\n",
      "Iteration:  73  Cost: 0.7074\n",
      "Iteration:  74  Cost: 0.5912\n",
      "Iteration:  75  Cost: 0.6898\n",
      "Iteration:  76  Cost: 0.5826\n",
      "Iteration:  77  Cost: 0.6725\n",
      "Iteration:  78  Cost: 0.5740\n",
      "Iteration:  79  Cost: 0.6554\n",
      "Iteration:  80  Cost: 0.5653\n",
      "Iteration:  81  Cost: 0.6386\n",
      "Iteration:  82  Cost: 0.5565\n",
      "Iteration:  83  Cost: 0.6221\n",
      "Iteration:  84  Cost: 0.5476\n",
      "Iteration:  85  Cost: 0.6058\n",
      "Iteration:  86  Cost: 0.5386\n",
      "Iteration:  87  Cost: 0.5898\n",
      "Iteration:  88  Cost: 0.5294\n",
      "Iteration:  89  Cost: 0.5740\n",
      "Iteration:  90  Cost: 0.5202\n",
      "Iteration:  91  Cost: 0.5586\n",
      "Iteration:  92  Cost: 0.5109\n",
      "Iteration:  93  Cost: 0.5435\n",
      "Iteration:  94  Cost: 0.5015\n",
      "Iteration:  95  Cost: 0.5287\n",
      "Iteration:  96  Cost: 0.4920\n",
      "Iteration:  97  Cost: 0.5143\n",
      "Iteration:  98  Cost: 0.4826\n",
      "Iteration:  99  Cost: 0.5005\n",
      "Iteration:  100  Cost: 0.4732\n",
      "Iteration:  101  Cost: 0.4871\n",
      "Iteration:  102  Cost: 0.4640\n",
      "Iteration:  103  Cost: 0.4745\n",
      "Iteration:  104  Cost: 0.4551\n",
      "Iteration:  105  Cost: 0.4626\n",
      "Iteration:  106  Cost: 0.4465\n",
      "Iteration:  107  Cost: 0.4516\n",
      "Iteration:  108  Cost: 0.4384\n",
      "Iteration:  109  Cost: 0.4415\n",
      "Iteration:  110  Cost: 0.4307\n",
      "Iteration:  111  Cost: 0.4323\n",
      "Iteration:  112  Cost: 0.4237\n",
      "Iteration:  113  Cost: 0.4242\n",
      "Iteration:  114  Cost: 0.4174\n",
      "Iteration:  115  Cost: 0.4171\n",
      "Iteration:  116  Cost: 0.4117\n",
      "Iteration:  117  Cost: 0.4109\n",
      "Iteration:  118  Cost: 0.4067\n",
      "Iteration:  119  Cost: 0.4056\n",
      "Iteration:  120  Cost: 0.4024\n",
      "Iteration:  121  Cost: 0.4012\n",
      "Iteration:  122  Cost: 0.3986\n",
      "Iteration:  123  Cost: 0.3975\n",
      "Iteration:  124  Cost: 0.3955\n",
      "Iteration:  125  Cost: 0.3944\n",
      "Iteration:  126  Cost: 0.3928\n",
      "Iteration:  127  Cost: 0.3918\n",
      "Iteration:  128  Cost: 0.3905\n",
      "Iteration:  129  Cost: 0.3896\n",
      "Iteration:  130  Cost: 0.3886\n",
      "Iteration:  131  Cost: 0.3878\n",
      "Iteration:  132  Cost: 0.3869\n",
      "Iteration:  133  Cost: 0.3862\n",
      "Iteration:  134  Cost: 0.3855\n",
      "Iteration:  135  Cost: 0.3848\n",
      "Iteration:  136  Cost: 0.3842\n",
      "Iteration:  137  Cost: 0.3836\n",
      "Iteration:  138  Cost: 0.3830\n",
      "Iteration:  139  Cost: 0.3824\n",
      "Iteration:  140  Cost: 0.3818\n",
      "Iteration:  141  Cost: 0.3813\n",
      "Iteration:  142  Cost: 0.3807\n",
      "Iteration:  143  Cost: 0.3802\n",
      "Iteration:  144  Cost: 0.3797\n",
      "Iteration:  145  Cost: 0.3792\n",
      "Iteration:  146  Cost: 0.3787\n",
      "Iteration:  147  Cost: 0.3782\n",
      "Iteration:  148  Cost: 0.3777\n",
      "Iteration:  149  Cost: 0.3772\n",
      "Iteration:  150  Cost: 0.3767\n",
      "Iteration:  151  Cost: 0.3762\n",
      "Iteration:  152  Cost: 0.3757\n",
      "Iteration:  153  Cost: 0.3753\n",
      "Iteration:  154  Cost: 0.3748\n",
      "Iteration:  155  Cost: 0.3743\n",
      "Iteration:  156  Cost: 0.3739\n",
      "Iteration:  157  Cost: 0.3734\n",
      "Iteration:  158  Cost: 0.3729\n",
      "Iteration:  159  Cost: 0.3725\n",
      "Iteration:  160  Cost: 0.3720\n",
      "Iteration:  161  Cost: 0.3715\n",
      "Iteration:  162  Cost: 0.3711\n",
      "Iteration:  163  Cost: 0.3706\n",
      "Iteration:  164  Cost: 0.3702\n",
      "Iteration:  165  Cost: 0.3697\n",
      "Iteration:  166  Cost: 0.3693\n",
      "Iteration:  167  Cost: 0.3688\n",
      "Iteration:  168  Cost: 0.3684\n",
      "Iteration:  169  Cost: 0.3679\n",
      "Iteration:  170  Cost: 0.3675\n",
      "Iteration:  171  Cost: 0.3671\n",
      "Iteration:  172  Cost: 0.3666\n",
      "Iteration:  173  Cost: 0.3662\n",
      "Iteration:  174  Cost: 0.3658\n",
      "Iteration:  175  Cost: 0.3653\n",
      "Iteration:  176  Cost: 0.3649\n",
      "Iteration:  177  Cost: 0.3645\n",
      "Iteration:  178  Cost: 0.3641\n",
      "Iteration:  179  Cost: 0.3636\n",
      "Iteration:  180  Cost: 0.3632\n",
      "Iteration:  181  Cost: 0.3628\n",
      "Iteration:  182  Cost: 0.3624\n",
      "Iteration:  183  Cost: 0.3620\n",
      "Iteration:  184  Cost: 0.3615\n",
      "Iteration:  185  Cost: 0.3611\n",
      "Iteration:  186  Cost: 0.3607\n",
      "Iteration:  187  Cost: 0.3603\n",
      "Iteration:  188  Cost: 0.3599\n",
      "Iteration:  189  Cost: 0.3595\n",
      "Iteration:  190  Cost: 0.3591\n",
      "Iteration:  191  Cost: 0.3587\n",
      "Iteration:  192  Cost: 0.3583\n",
      "Iteration:  193  Cost: 0.3579\n",
      "Iteration:  194  Cost: 0.3575\n",
      "Iteration:  195  Cost: 0.3571\n",
      "Iteration:  196  Cost: 0.3567\n",
      "Iteration:  197  Cost: 0.3563\n",
      "Iteration:  198  Cost: 0.3560\n",
      "Iteration:  199  Cost: 0.3556\n",
      "Iteration:  200  Cost: 0.3552\n",
      "Iteration:  201  Cost: 0.3548\n",
      "Iteration:  202  Cost: 0.3544\n",
      "Iteration:  203  Cost: 0.3540\n",
      "Iteration:  204  Cost: 0.3537\n",
      "Iteration:  205  Cost: 0.3533\n",
      "Iteration:  206  Cost: 0.3529\n",
      "Iteration:  207  Cost: 0.3525\n",
      "Iteration:  208  Cost: 0.3522\n",
      "Iteration:  209  Cost: 0.3518\n",
      "Iteration:  210  Cost: 0.3514\n",
      "Iteration:  211  Cost: 0.3511\n",
      "Iteration:  212  Cost: 0.3507\n",
      "Iteration:  213  Cost: 0.3503\n",
      "Iteration:  214  Cost: 0.3500\n",
      "Iteration:  215  Cost: 0.3496\n",
      "Iteration:  216  Cost: 0.3492\n",
      "Iteration:  217  Cost: 0.3489\n",
      "Iteration:  218  Cost: 0.3485\n",
      "Iteration:  219  Cost: 0.3482\n",
      "Iteration:  220  Cost: 0.3478\n",
      "Iteration:  221  Cost: 0.3475\n",
      "Iteration:  222  Cost: 0.3471\n",
      "Iteration:  223  Cost: 0.3468\n",
      "Iteration:  224  Cost: 0.3464\n",
      "Iteration:  225  Cost: 0.3461\n",
      "Iteration:  226  Cost: 0.3457\n",
      "Iteration:  227  Cost: 0.3454\n",
      "Iteration:  228  Cost: 0.3450\n",
      "Iteration:  229  Cost: 0.3447\n",
      "Iteration:  230  Cost: 0.3443\n",
      "Iteration:  231  Cost: 0.3440\n",
      "Iteration:  232  Cost: 0.3437\n",
      "Iteration:  233  Cost: 0.3433\n",
      "Iteration:  234  Cost: 0.3430\n",
      "Iteration:  235  Cost: 0.3427\n",
      "Iteration:  236  Cost: 0.3423\n",
      "Iteration:  237  Cost: 0.3420\n",
      "Iteration:  238  Cost: 0.3417\n",
      "Iteration:  239  Cost: 0.3413\n",
      "Iteration:  240  Cost: 0.3410\n",
      "Iteration:  241  Cost: 0.3407\n",
      "Iteration:  242  Cost: 0.3404\n",
      "Iteration:  243  Cost: 0.3400\n",
      "Iteration:  244  Cost: 0.3397\n",
      "Iteration:  245  Cost: 0.3394\n",
      "Iteration:  246  Cost: 0.3391\n",
      "Iteration:  247  Cost: 0.3387\n",
      "Iteration:  248  Cost: 0.3384\n",
      "Iteration:  249  Cost: 0.3381\n",
      "Iteration:  250  Cost: 0.3378\n",
      "Iteration:  251  Cost: 0.3375\n",
      "Iteration:  252  Cost: 0.3372\n",
      "Iteration:  253  Cost: 0.3369\n",
      "Iteration:  254  Cost: 0.3365\n",
      "Iteration:  255  Cost: 0.3362\n",
      "Iteration:  256  Cost: 0.3359\n",
      "Iteration:  257  Cost: 0.3356\n",
      "Iteration:  258  Cost: 0.3353\n",
      "Iteration:  259  Cost: 0.3350\n",
      "Iteration:  260  Cost: 0.3347\n",
      "Iteration:  261  Cost: 0.3344\n",
      "Iteration:  262  Cost: 0.3341\n",
      "Iteration:  263  Cost: 0.3338\n",
      "Iteration:  264  Cost: 0.3335\n",
      "Iteration:  265  Cost: 0.3332\n",
      "Iteration:  266  Cost: 0.3329\n",
      "Iteration:  267  Cost: 0.3326\n",
      "Iteration:  268  Cost: 0.3323\n",
      "Iteration:  269  Cost: 0.3320\n",
      "Iteration:  270  Cost: 0.3317\n",
      "Iteration:  271  Cost: 0.3314\n",
      "Iteration:  272  Cost: 0.3311\n",
      "Iteration:  273  Cost: 0.3308\n",
      "Iteration:  274  Cost: 0.3305\n",
      "Iteration:  275  Cost: 0.3303\n",
      "Iteration:  276  Cost: 0.3300\n",
      "Iteration:  277  Cost: 0.3297\n",
      "Iteration:  278  Cost: 0.3294\n",
      "Iteration:  279  Cost: 0.3291\n",
      "Iteration:  280  Cost: 0.3288\n",
      "Iteration:  281  Cost: 0.3285\n",
      "Iteration:  282  Cost: 0.3283\n",
      "Iteration:  283  Cost: 0.3280\n",
      "Iteration:  284  Cost: 0.3277\n",
      "Iteration:  285  Cost: 0.3274\n",
      "Iteration:  286  Cost: 0.3271\n",
      "Iteration:  287  Cost: 0.3269\n",
      "Iteration:  288  Cost: 0.3266\n",
      "Iteration:  289  Cost: 0.3263\n",
      "Iteration:  290  Cost: 0.3260\n",
      "Iteration:  291  Cost: 0.3258\n",
      "Iteration:  292  Cost: 0.3255\n",
      "Iteration:  293  Cost: 0.3252\n",
      "Iteration:  294  Cost: 0.3250\n",
      "Iteration:  295  Cost: 0.3247\n",
      "Iteration:  296  Cost: 0.3244\n",
      "Iteration:  297  Cost: 0.3242\n",
      "Iteration:  298  Cost: 0.3239\n",
      "Iteration:  299  Cost: 0.3236\n",
      "Iteration:  300  Cost: 0.3234\n",
      "Iteration:  301  Cost: 0.3231\n",
      "Iteration:  302  Cost: 0.3228\n",
      "Iteration:  303  Cost: 0.3226\n",
      "Iteration:  304  Cost: 0.3223\n",
      "Iteration:  305  Cost: 0.3220\n",
      "Iteration:  306  Cost: 0.3218\n",
      "Iteration:  307  Cost: 0.3215\n",
      "Iteration:  308  Cost: 0.3213\n",
      "Iteration:  309  Cost: 0.3210\n",
      "Iteration:  310  Cost: 0.3208\n",
      "Iteration:  311  Cost: 0.3205\n",
      "Iteration:  312  Cost: 0.3202\n",
      "Iteration:  313  Cost: 0.3200\n",
      "Iteration:  314  Cost: 0.3197\n",
      "Iteration:  315  Cost: 0.3195\n",
      "Iteration:  316  Cost: 0.3192\n",
      "Iteration:  317  Cost: 0.3190\n",
      "Iteration:  318  Cost: 0.3187\n",
      "Iteration:  319  Cost: 0.3185\n",
      "Iteration:  320  Cost: 0.3182\n",
      "Iteration:  321  Cost: 0.3180\n",
      "Iteration:  322  Cost: 0.3177\n",
      "Iteration:  323  Cost: 0.3175\n",
      "Iteration:  324  Cost: 0.3172\n",
      "Iteration:  325  Cost: 0.3170\n",
      "Iteration:  326  Cost: 0.3168\n",
      "Iteration:  327  Cost: 0.3165\n",
      "Iteration:  328  Cost: 0.3163\n",
      "Iteration:  329  Cost: 0.3160\n",
      "Iteration:  330  Cost: 0.3158\n",
      "Iteration:  331  Cost: 0.3156\n",
      "Iteration:  332  Cost: 0.3153\n",
      "Iteration:  333  Cost: 0.3151\n",
      "Iteration:  334  Cost: 0.3148\n",
      "Iteration:  335  Cost: 0.3146\n",
      "Iteration:  336  Cost: 0.3144\n",
      "Iteration:  337  Cost: 0.3141\n",
      "Iteration:  338  Cost: 0.3139\n",
      "Iteration:  339  Cost: 0.3137\n",
      "Iteration:  340  Cost: 0.3134\n",
      "Iteration:  341  Cost: 0.3132\n",
      "Iteration:  342  Cost: 0.3130\n",
      "Iteration:  343  Cost: 0.3127\n",
      "Iteration:  344  Cost: 0.3125\n",
      "Iteration:  345  Cost: 0.3123\n",
      "Iteration:  346  Cost: 0.3120\n",
      "Iteration:  347  Cost: 0.3118\n",
      "Iteration:  348  Cost: 0.3116\n",
      "Iteration:  349  Cost: 0.3114\n",
      "Iteration:  350  Cost: 0.3111\n",
      "Iteration:  351  Cost: 0.3109\n",
      "Iteration:  352  Cost: 0.3107\n",
      "Iteration:  353  Cost: 0.3105\n",
      "Iteration:  354  Cost: 0.3102\n",
      "Iteration:  355  Cost: 0.3100\n",
      "Iteration:  356  Cost: 0.3098\n",
      "Iteration:  357  Cost: 0.3096\n",
      "Iteration:  358  Cost: 0.3094\n",
      "Iteration:  359  Cost: 0.3091\n",
      "Iteration:  360  Cost: 0.3089\n",
      "Iteration:  361  Cost: 0.3087\n",
      "Iteration:  362  Cost: 0.3085\n",
      "Iteration:  363  Cost: 0.3083\n",
      "Iteration:  364  Cost: 0.3081\n",
      "Iteration:  365  Cost: 0.3078\n",
      "Iteration:  366  Cost: 0.3076\n",
      "Iteration:  367  Cost: 0.3074\n",
      "Iteration:  368  Cost: 0.3072\n",
      "Iteration:  369  Cost: 0.3070\n",
      "Iteration:  370  Cost: 0.3068\n",
      "Iteration:  371  Cost: 0.3066\n",
      "Iteration:  372  Cost: 0.3063\n",
      "Iteration:  373  Cost: 0.3061\n",
      "Iteration:  374  Cost: 0.3059\n",
      "Iteration:  375  Cost: 0.3057\n",
      "Iteration:  376  Cost: 0.3055\n",
      "Iteration:  377  Cost: 0.3053\n",
      "Iteration:  378  Cost: 0.3051\n",
      "Iteration:  379  Cost: 0.3049\n",
      "Iteration:  380  Cost: 0.3047\n",
      "Iteration:  381  Cost: 0.3045\n",
      "Iteration:  382  Cost: 0.3043\n",
      "Iteration:  383  Cost: 0.3041\n",
      "Iteration:  384  Cost: 0.3039\n",
      "Iteration:  385  Cost: 0.3037\n",
      "Iteration:  386  Cost: 0.3035\n",
      "Iteration:  387  Cost: 0.3033\n",
      "Iteration:  388  Cost: 0.3031\n",
      "Iteration:  389  Cost: 0.3029\n",
      "Iteration:  390  Cost: 0.3027\n",
      "Iteration:  391  Cost: 0.3025\n",
      "Iteration:  392  Cost: 0.3023\n",
      "Iteration:  393  Cost: 0.3021\n",
      "Iteration:  394  Cost: 0.3019\n",
      "Iteration:  395  Cost: 0.3017\n",
      "Iteration:  396  Cost: 0.3015\n",
      "Iteration:  397  Cost: 0.3013\n",
      "Iteration:  398  Cost: 0.3011\n",
      "Iteration:  399  Cost: 0.3009\n",
      "Iteration:  400  Cost: 0.3007\n",
      "Iteration:  401  Cost: 0.3005\n",
      "Iteration:  402  Cost: 0.3003\n",
      "Iteration:  403  Cost: 0.3001\n",
      "Iteration:  404  Cost: 0.2999\n",
      "Iteration:  405  Cost: 0.2997\n",
      "Iteration:  406  Cost: 0.2995\n",
      "Iteration:  407  Cost: 0.2993\n",
      "Iteration:  408  Cost: 0.2991\n",
      "Iteration:  409  Cost: 0.2990\n",
      "Iteration:  410  Cost: 0.2988\n",
      "Iteration:  411  Cost: 0.2986\n",
      "Iteration:  412  Cost: 0.2984\n",
      "Iteration:  413  Cost: 0.2982\n",
      "Iteration:  414  Cost: 0.2980\n",
      "Iteration:  415  Cost: 0.2978\n",
      "Iteration:  416  Cost: 0.2976\n",
      "Iteration:  417  Cost: 0.2975\n",
      "Iteration:  418  Cost: 0.2973\n",
      "Iteration:  419  Cost: 0.2971\n",
      "Iteration:  420  Cost: 0.2969\n",
      "Iteration:  421  Cost: 0.2967\n",
      "Iteration:  422  Cost: 0.2965\n",
      "Iteration:  423  Cost: 0.2964\n",
      "Iteration:  424  Cost: 0.2962\n",
      "Iteration:  425  Cost: 0.2960\n",
      "Iteration:  426  Cost: 0.2958\n",
      "Iteration:  427  Cost: 0.2956\n",
      "Iteration:  428  Cost: 0.2955\n",
      "Iteration:  429  Cost: 0.2953\n",
      "Iteration:  430  Cost: 0.2951\n",
      "Iteration:  431  Cost: 0.2949\n",
      "Iteration:  432  Cost: 0.2948\n",
      "Iteration:  433  Cost: 0.2946\n",
      "Iteration:  434  Cost: 0.2944\n",
      "Iteration:  435  Cost: 0.2942\n",
      "Iteration:  436  Cost: 0.2940\n",
      "Iteration:  437  Cost: 0.2939\n",
      "Iteration:  438  Cost: 0.2937\n",
      "Iteration:  439  Cost: 0.2935\n",
      "Iteration:  440  Cost: 0.2934\n",
      "Iteration:  441  Cost: 0.2932\n",
      "Iteration:  442  Cost: 0.2930\n",
      "Iteration:  443  Cost: 0.2928\n",
      "Iteration:  444  Cost: 0.2927\n",
      "Iteration:  445  Cost: 0.2925\n",
      "Iteration:  446  Cost: 0.2923\n",
      "Iteration:  447  Cost: 0.2922\n",
      "Iteration:  448  Cost: 0.2920\n",
      "Iteration:  449  Cost: 0.2918\n",
      "Iteration:  450  Cost: 0.2916\n",
      "Iteration:  451  Cost: 0.2915\n",
      "Iteration:  452  Cost: 0.2913\n",
      "Iteration:  453  Cost: 0.2911\n",
      "Iteration:  454  Cost: 0.2910\n",
      "Iteration:  455  Cost: 0.2908\n",
      "Iteration:  456  Cost: 0.2906\n",
      "Iteration:  457  Cost: 0.2905\n",
      "Iteration:  458  Cost: 0.2903\n",
      "Iteration:  459  Cost: 0.2901\n",
      "Iteration:  460  Cost: 0.2900\n",
      "Iteration:  461  Cost: 0.2898\n",
      "Iteration:  462  Cost: 0.2897\n",
      "Iteration:  463  Cost: 0.2895\n",
      "Iteration:  464  Cost: 0.2893\n",
      "Iteration:  465  Cost: 0.2892\n",
      "Iteration:  466  Cost: 0.2890\n",
      "Iteration:  467  Cost: 0.2888\n",
      "Iteration:  468  Cost: 0.2887\n",
      "Iteration:  469  Cost: 0.2885\n",
      "Iteration:  470  Cost: 0.2884\n",
      "Iteration:  471  Cost: 0.2882\n",
      "Iteration:  472  Cost: 0.2880\n",
      "Iteration:  473  Cost: 0.2879\n",
      "Iteration:  474  Cost: 0.2877\n",
      "Iteration:  475  Cost: 0.2876\n",
      "Iteration:  476  Cost: 0.2874\n",
      "Iteration:  477  Cost: 0.2873\n",
      "Iteration:  478  Cost: 0.2871\n",
      "Iteration:  479  Cost: 0.2869\n",
      "Iteration:  480  Cost: 0.2868\n",
      "Iteration:  481  Cost: 0.2866\n",
      "Iteration:  482  Cost: 0.2865\n",
      "Iteration:  483  Cost: 0.2863\n",
      "Iteration:  484  Cost: 0.2862\n",
      "Iteration:  485  Cost: 0.2860\n",
      "Iteration:  486  Cost: 0.2859\n",
      "Iteration:  487  Cost: 0.2857\n",
      "Iteration:  488  Cost: 0.2856\n",
      "Iteration:  489  Cost: 0.2854\n",
      "Iteration:  490  Cost: 0.2853\n",
      "Iteration:  491  Cost: 0.2851\n",
      "Iteration:  492  Cost: 0.2850\n",
      "Iteration:  493  Cost: 0.2848\n",
      "Iteration:  494  Cost: 0.2847\n",
      "Iteration:  495  Cost: 0.2845\n",
      "Iteration:  496  Cost: 0.2844\n",
      "Iteration:  497  Cost: 0.2842\n",
      "Iteration:  498  Cost: 0.2841\n",
      "Iteration:  499  Cost: 0.2839\n",
      "Iteration:  500  Cost: 0.2838\n",
      "Iteration:  501  Cost: 0.2836\n",
      "Iteration:  502  Cost: 0.2835\n",
      "Iteration:  503  Cost: 0.2833\n",
      "Iteration:  504  Cost: 0.2832\n",
      "Iteration:  505  Cost: 0.2830\n",
      "Iteration:  506  Cost: 0.2829\n",
      "Iteration:  507  Cost: 0.2828\n",
      "Iteration:  508  Cost: 0.2826\n",
      "Iteration:  509  Cost: 0.2825\n",
      "Iteration:  510  Cost: 0.2823\n",
      "Iteration:  511  Cost: 0.2822\n",
      "Iteration:  512  Cost: 0.2820\n",
      "Iteration:  513  Cost: 0.2819\n",
      "Iteration:  514  Cost: 0.2818\n",
      "Iteration:  515  Cost: 0.2816\n",
      "Iteration:  516  Cost: 0.2815\n",
      "Iteration:  517  Cost: 0.2813\n",
      "Iteration:  518  Cost: 0.2812\n",
      "Iteration:  519  Cost: 0.2810\n",
      "Iteration:  520  Cost: 0.2809\n",
      "Iteration:  521  Cost: 0.2808\n",
      "Iteration:  522  Cost: 0.2806\n",
      "Iteration:  523  Cost: 0.2805\n",
      "Iteration:  524  Cost: 0.2804\n",
      "Iteration:  525  Cost: 0.2802\n",
      "Iteration:  526  Cost: 0.2801\n",
      "Iteration:  527  Cost: 0.2799\n",
      "Iteration:  528  Cost: 0.2798\n",
      "Iteration:  529  Cost: 0.2797\n",
      "Iteration:  530  Cost: 0.2795\n",
      "Iteration:  531  Cost: 0.2794\n",
      "Iteration:  532  Cost: 0.2793\n",
      "Iteration:  533  Cost: 0.2791\n",
      "Iteration:  534  Cost: 0.2790\n",
      "Iteration:  535  Cost: 0.2789\n",
      "Iteration:  536  Cost: 0.2787\n",
      "Iteration:  537  Cost: 0.2786\n",
      "Iteration:  538  Cost: 0.2785\n",
      "Iteration:  539  Cost: 0.2783\n",
      "Iteration:  540  Cost: 0.2782\n",
      "Iteration:  541  Cost: 0.2781\n",
      "Iteration:  542  Cost: 0.2779\n",
      "Iteration:  543  Cost: 0.2778\n",
      "Iteration:  544  Cost: 0.2777\n",
      "Iteration:  545  Cost: 0.2775\n",
      "Iteration:  546  Cost: 0.2774\n",
      "Iteration:  547  Cost: 0.2773\n",
      "Iteration:  548  Cost: 0.2771\n",
      "Iteration:  549  Cost: 0.2770\n",
      "Iteration:  550  Cost: 0.2769\n",
      "Iteration:  551  Cost: 0.2767\n",
      "Iteration:  552  Cost: 0.2766\n",
      "Iteration:  553  Cost: 0.2765\n",
      "Iteration:  554  Cost: 0.2764\n",
      "Iteration:  555  Cost: 0.2762\n",
      "Iteration:  556  Cost: 0.2761\n",
      "Iteration:  557  Cost: 0.2760\n",
      "Iteration:  558  Cost: 0.2759\n",
      "Iteration:  559  Cost: 0.2757\n",
      "Iteration:  560  Cost: 0.2756\n",
      "Iteration:  561  Cost: 0.2755\n",
      "Iteration:  562  Cost: 0.2754\n",
      "Iteration:  563  Cost: 0.2752\n",
      "Iteration:  564  Cost: 0.2751\n",
      "Iteration:  565  Cost: 0.2750\n",
      "Iteration:  566  Cost: 0.2749\n",
      "Iteration:  567  Cost: 0.2747\n",
      "Iteration:  568  Cost: 0.2746\n",
      "Iteration:  569  Cost: 0.2745\n",
      "Iteration:  570  Cost: 0.2744\n",
      "Iteration:  571  Cost: 0.2742\n",
      "Iteration:  572  Cost: 0.2741\n",
      "Iteration:  573  Cost: 0.2740\n",
      "Iteration:  574  Cost: 0.2739\n",
      "Iteration:  575  Cost: 0.2737\n",
      "Iteration:  576  Cost: 0.2736\n",
      "Iteration:  577  Cost: 0.2735\n",
      "Iteration:  578  Cost: 0.2734\n",
      "Iteration:  579  Cost: 0.2733\n",
      "Iteration:  580  Cost: 0.2731\n",
      "Iteration:  581  Cost: 0.2730\n",
      "Iteration:  582  Cost: 0.2729\n",
      "Iteration:  583  Cost: 0.2728\n",
      "Iteration:  584  Cost: 0.2727\n",
      "Iteration:  585  Cost: 0.2725\n",
      "Iteration:  586  Cost: 0.2724\n",
      "Iteration:  587  Cost: 0.2723\n",
      "Iteration:  588  Cost: 0.2722\n",
      "Iteration:  589  Cost: 0.2721\n",
      "Iteration:  590  Cost: 0.2720\n",
      "Iteration:  591  Cost: 0.2718\n",
      "Iteration:  592  Cost: 0.2717\n",
      "Iteration:  593  Cost: 0.2716\n",
      "Iteration:  594  Cost: 0.2715\n",
      "Iteration:  595  Cost: 0.2714\n",
      "Iteration:  596  Cost: 0.2713\n",
      "Iteration:  597  Cost: 0.2711\n",
      "Iteration:  598  Cost: 0.2710\n",
      "Iteration:  599  Cost: 0.2709\n",
      "Iteration:  600  Cost: 0.2708\n",
      "Iteration:  601  Cost: 0.2707\n",
      "Iteration:  602  Cost: 0.2706\n",
      "Iteration:  603  Cost: 0.2705\n",
      "Iteration:  604  Cost: 0.2703\n",
      "Iteration:  605  Cost: 0.2702\n",
      "Iteration:  606  Cost: 0.2701\n",
      "Iteration:  607  Cost: 0.2700\n",
      "Iteration:  608  Cost: 0.2699\n",
      "Iteration:  609  Cost: 0.2698\n",
      "Iteration:  610  Cost: 0.2697\n",
      "Iteration:  611  Cost: 0.2696\n",
      "Iteration:  612  Cost: 0.2694\n",
      "Iteration:  613  Cost: 0.2693\n",
      "Iteration:  614  Cost: 0.2692\n",
      "Iteration:  615  Cost: 0.2691\n",
      "Iteration:  616  Cost: 0.2690\n",
      "Iteration:  617  Cost: 0.2689\n",
      "Iteration:  618  Cost: 0.2688\n",
      "Iteration:  619  Cost: 0.2687\n",
      "Iteration:  620  Cost: 0.2686\n",
      "Iteration:  621  Cost: 0.2685\n",
      "Iteration:  622  Cost: 0.2683\n",
      "Iteration:  623  Cost: 0.2682\n",
      "Iteration:  624  Cost: 0.2681\n",
      "Iteration:  625  Cost: 0.2680\n",
      "Iteration:  626  Cost: 0.2679\n",
      "Iteration:  627  Cost: 0.2678\n",
      "Iteration:  628  Cost: 0.2677\n",
      "Iteration:  629  Cost: 0.2676\n",
      "Iteration:  630  Cost: 0.2675\n",
      "Iteration:  631  Cost: 0.2674\n",
      "Iteration:  632  Cost: 0.2673\n",
      "Iteration:  633  Cost: 0.2672\n",
      "Iteration:  634  Cost: 0.2671\n",
      "Iteration:  635  Cost: 0.2670\n",
      "Iteration:  636  Cost: 0.2668\n",
      "Iteration:  637  Cost: 0.2667\n",
      "Iteration:  638  Cost: 0.2666\n",
      "Iteration:  639  Cost: 0.2665\n",
      "Iteration:  640  Cost: 0.2664\n",
      "Iteration:  641  Cost: 0.2663\n",
      "Iteration:  642  Cost: 0.2662\n",
      "Iteration:  643  Cost: 0.2661\n",
      "Iteration:  644  Cost: 0.2660\n",
      "Iteration:  645  Cost: 0.2659\n",
      "Iteration:  646  Cost: 0.2658\n",
      "Iteration:  647  Cost: 0.2657\n",
      "Iteration:  648  Cost: 0.2656\n",
      "Iteration:  649  Cost: 0.2655\n",
      "Iteration:  650  Cost: 0.2654\n",
      "Iteration:  651  Cost: 0.2653\n",
      "Iteration:  652  Cost: 0.2652\n",
      "Iteration:  653  Cost: 0.2651\n",
      "Iteration:  654  Cost: 0.2650\n",
      "Iteration:  655  Cost: 0.2649\n",
      "Iteration:  656  Cost: 0.2648\n",
      "Iteration:  657  Cost: 0.2647\n",
      "Iteration:  658  Cost: 0.2646\n",
      "Iteration:  659  Cost: 0.2645\n",
      "Iteration:  660  Cost: 0.2644\n",
      "Iteration:  661  Cost: 0.2643\n",
      "Iteration:  662  Cost: 0.2642\n",
      "Iteration:  663  Cost: 0.2641\n",
      "Iteration:  664  Cost: 0.2640\n",
      "Iteration:  665  Cost: 0.2639\n",
      "Iteration:  666  Cost: 0.2638\n",
      "Iteration:  667  Cost: 0.2637\n",
      "Iteration:  668  Cost: 0.2636\n",
      "Iteration:  669  Cost: 0.2635\n",
      "Iteration:  670  Cost: 0.2634\n",
      "Iteration:  671  Cost: 0.2633\n",
      "Iteration:  672  Cost: 0.2632\n",
      "Iteration:  673  Cost: 0.2631\n",
      "Iteration:  674  Cost: 0.2630\n",
      "Iteration:  675  Cost: 0.2629\n",
      "Iteration:  676  Cost: 0.2628\n",
      "Iteration:  677  Cost: 0.2627\n",
      "Iteration:  678  Cost: 0.2626\n",
      "Iteration:  679  Cost: 0.2625\n",
      "Iteration:  680  Cost: 0.2624\n",
      "Iteration:  681  Cost: 0.2623\n",
      "Iteration:  682  Cost: 0.2623\n",
      "Iteration:  683  Cost: 0.2622\n",
      "Iteration:  684  Cost: 0.2621\n",
      "Iteration:  685  Cost: 0.2620\n",
      "Iteration:  686  Cost: 0.2619\n",
      "Iteration:  687  Cost: 0.2618\n",
      "Iteration:  688  Cost: 0.2617\n",
      "Iteration:  689  Cost: 0.2616\n",
      "Iteration:  690  Cost: 0.2615\n",
      "Iteration:  691  Cost: 0.2614\n",
      "Iteration:  692  Cost: 0.2613\n",
      "Iteration:  693  Cost: 0.2612\n",
      "Iteration:  694  Cost: 0.2611\n",
      "Iteration:  695  Cost: 0.2610\n",
      "Iteration:  696  Cost: 0.2609\n",
      "Iteration:  697  Cost: 0.2609\n",
      "Iteration:  698  Cost: 0.2608\n",
      "Iteration:  699  Cost: 0.2607\n",
      "Iteration:  700  Cost: 0.2606\n",
      "Iteration:  701  Cost: 0.2605\n",
      "Iteration:  702  Cost: 0.2604\n",
      "Iteration:  703  Cost: 0.2603\n",
      "Iteration:  704  Cost: 0.2602\n",
      "Iteration:  705  Cost: 0.2601\n",
      "Iteration:  706  Cost: 0.2600\n",
      "Iteration:  707  Cost: 0.2599\n",
      "Iteration:  708  Cost: 0.2599\n",
      "Iteration:  709  Cost: 0.2598\n",
      "Iteration:  710  Cost: 0.2597\n",
      "Iteration:  711  Cost: 0.2596\n",
      "Iteration:  712  Cost: 0.2595\n",
      "Iteration:  713  Cost: 0.2594\n",
      "Iteration:  714  Cost: 0.2593\n",
      "Iteration:  715  Cost: 0.2592\n",
      "Iteration:  716  Cost: 0.2591\n",
      "Iteration:  717  Cost: 0.2591\n",
      "Iteration:  718  Cost: 0.2590\n",
      "Iteration:  719  Cost: 0.2589\n",
      "Iteration:  720  Cost: 0.2588\n",
      "Iteration:  721  Cost: 0.2587\n",
      "Iteration:  722  Cost: 0.2586\n",
      "Iteration:  723  Cost: 0.2585\n",
      "Iteration:  724  Cost: 0.2584\n",
      "Iteration:  725  Cost: 0.2584\n",
      "Iteration:  726  Cost: 0.2583\n",
      "Iteration:  727  Cost: 0.2582\n",
      "Iteration:  728  Cost: 0.2581\n",
      "Iteration:  729  Cost: 0.2580\n",
      "Iteration:  730  Cost: 0.2579\n",
      "Iteration:  731  Cost: 0.2578\n",
      "Iteration:  732  Cost: 0.2578\n",
      "Iteration:  733  Cost: 0.2577\n",
      "Iteration:  734  Cost: 0.2576\n",
      "Iteration:  735  Cost: 0.2575\n",
      "Iteration:  736  Cost: 0.2574\n",
      "Iteration:  737  Cost: 0.2573\n",
      "Iteration:  738  Cost: 0.2572\n",
      "Iteration:  739  Cost: 0.2572\n",
      "Iteration:  740  Cost: 0.2571\n",
      "Iteration:  741  Cost: 0.2570\n",
      "Iteration:  742  Cost: 0.2569\n",
      "Iteration:  743  Cost: 0.2568\n",
      "Iteration:  744  Cost: 0.2567\n",
      "Iteration:  745  Cost: 0.2567\n",
      "Iteration:  746  Cost: 0.2566\n",
      "Iteration:  747  Cost: 0.2565\n",
      "Iteration:  748  Cost: 0.2564\n",
      "Iteration:  749  Cost: 0.2563\n",
      "Iteration:  750  Cost: 0.2562\n",
      "Iteration:  751  Cost: 0.2562\n",
      "Iteration:  752  Cost: 0.2561\n",
      "Iteration:  753  Cost: 0.2560\n",
      "Iteration:  754  Cost: 0.2559\n",
      "Iteration:  755  Cost: 0.2558\n",
      "Iteration:  756  Cost: 0.2558\n",
      "Iteration:  757  Cost: 0.2557\n",
      "Iteration:  758  Cost: 0.2556\n",
      "Iteration:  759  Cost: 0.2555\n",
      "Iteration:  760  Cost: 0.2554\n",
      "Iteration:  761  Cost: 0.2553\n",
      "Iteration:  762  Cost: 0.2553\n",
      "Iteration:  763  Cost: 0.2552\n",
      "Iteration:  764  Cost: 0.2551\n",
      "Iteration:  765  Cost: 0.2550\n",
      "Iteration:  766  Cost: 0.2549\n",
      "Iteration:  767  Cost: 0.2549\n",
      "Iteration:  768  Cost: 0.2548\n",
      "Iteration:  769  Cost: 0.2547\n",
      "Iteration:  770  Cost: 0.2546\n",
      "Iteration:  771  Cost: 0.2546\n",
      "Iteration:  772  Cost: 0.2545\n",
      "Iteration:  773  Cost: 0.2544\n",
      "Iteration:  774  Cost: 0.2543\n",
      "Iteration:  775  Cost: 0.2542\n",
      "Iteration:  776  Cost: 0.2542\n",
      "Iteration:  777  Cost: 0.2541\n",
      "Iteration:  778  Cost: 0.2540\n",
      "Iteration:  779  Cost: 0.2539\n",
      "Iteration:  780  Cost: 0.2538\n",
      "Iteration:  781  Cost: 0.2538\n",
      "Iteration:  782  Cost: 0.2537\n",
      "Iteration:  783  Cost: 0.2536\n",
      "Iteration:  784  Cost: 0.2535\n",
      "Iteration:  785  Cost: 0.2535\n",
      "Iteration:  786  Cost: 0.2534\n",
      "Iteration:  787  Cost: 0.2533\n",
      "Iteration:  788  Cost: 0.2532\n",
      "Iteration:  789  Cost: 0.2532\n",
      "Iteration:  790  Cost: 0.2531\n",
      "Iteration:  791  Cost: 0.2530\n",
      "Iteration:  792  Cost: 0.2529\n",
      "Iteration:  793  Cost: 0.2529\n",
      "Iteration:  794  Cost: 0.2528\n",
      "Iteration:  795  Cost: 0.2527\n",
      "Iteration:  796  Cost: 0.2526\n",
      "Iteration:  797  Cost: 0.2525\n",
      "Iteration:  798  Cost: 0.2525\n",
      "Iteration:  799  Cost: 0.2524\n",
      "Iteration:  800  Cost: 0.2523\n",
      "Iteration:  801  Cost: 0.2522\n",
      "Iteration:  802  Cost: 0.2522\n",
      "Iteration:  803  Cost: 0.2521\n",
      "Iteration:  804  Cost: 0.2520\n",
      "Iteration:  805  Cost: 0.2520\n",
      "Iteration:  806  Cost: 0.2519\n",
      "Iteration:  807  Cost: 0.2518\n",
      "Iteration:  808  Cost: 0.2517\n",
      "Iteration:  809  Cost: 0.2517\n",
      "Iteration:  810  Cost: 0.2516\n",
      "Iteration:  811  Cost: 0.2515\n",
      "Iteration:  812  Cost: 0.2514\n",
      "Iteration:  813  Cost: 0.2514\n",
      "Iteration:  814  Cost: 0.2513\n",
      "Iteration:  815  Cost: 0.2512\n",
      "Iteration:  816  Cost: 0.2511\n",
      "Iteration:  817  Cost: 0.2511\n",
      "Iteration:  818  Cost: 0.2510\n",
      "Iteration:  819  Cost: 0.2509\n",
      "Iteration:  820  Cost: 0.2509\n",
      "Iteration:  821  Cost: 0.2508\n",
      "Iteration:  822  Cost: 0.2507\n",
      "Iteration:  823  Cost: 0.2506\n",
      "Iteration:  824  Cost: 0.2506\n",
      "Iteration:  825  Cost: 0.2505\n",
      "Iteration:  826  Cost: 0.2504\n",
      "Iteration:  827  Cost: 0.2504\n",
      "Iteration:  828  Cost: 0.2503\n",
      "Iteration:  829  Cost: 0.2502\n",
      "Iteration:  830  Cost: 0.2501\n",
      "Iteration:  831  Cost: 0.2501\n",
      "Iteration:  832  Cost: 0.2500\n",
      "Iteration:  833  Cost: 0.2499\n",
      "Iteration:  834  Cost: 0.2499\n",
      "Iteration:  835  Cost: 0.2498\n",
      "Iteration:  836  Cost: 0.2497\n",
      "Iteration:  837  Cost: 0.2497\n",
      "Iteration:  838  Cost: 0.2496\n",
      "Iteration:  839  Cost: 0.2495\n",
      "Iteration:  840  Cost: 0.2494\n",
      "Iteration:  841  Cost: 0.2494\n",
      "Iteration:  842  Cost: 0.2493\n",
      "Iteration:  843  Cost: 0.2492\n",
      "Iteration:  844  Cost: 0.2492\n",
      "Iteration:  845  Cost: 0.2491\n",
      "Iteration:  846  Cost: 0.2490\n",
      "Iteration:  847  Cost: 0.2490\n",
      "Iteration:  848  Cost: 0.2489\n",
      "Iteration:  849  Cost: 0.2488\n",
      "Iteration:  850  Cost: 0.2488\n",
      "Iteration:  851  Cost: 0.2487\n",
      "Iteration:  852  Cost: 0.2486\n",
      "Iteration:  853  Cost: 0.2486\n",
      "Iteration:  854  Cost: 0.2485\n",
      "Iteration:  855  Cost: 0.2484\n",
      "Iteration:  856  Cost: 0.2484\n",
      "Iteration:  857  Cost: 0.2483\n",
      "Iteration:  858  Cost: 0.2482\n",
      "Iteration:  859  Cost: 0.2482\n",
      "Iteration:  860  Cost: 0.2481\n",
      "Iteration:  861  Cost: 0.2480\n",
      "Iteration:  862  Cost: 0.2480\n",
      "Iteration:  863  Cost: 0.2479\n",
      "Iteration:  864  Cost: 0.2478\n",
      "Iteration:  865  Cost: 0.2478\n",
      "Iteration:  866  Cost: 0.2477\n",
      "Iteration:  867  Cost: 0.2476\n",
      "Iteration:  868  Cost: 0.2476\n",
      "Iteration:  869  Cost: 0.2475\n",
      "Iteration:  870  Cost: 0.2474\n",
      "Iteration:  871  Cost: 0.2474\n",
      "Iteration:  872  Cost: 0.2473\n",
      "Iteration:  873  Cost: 0.2472\n",
      "Iteration:  874  Cost: 0.2472\n",
      "Iteration:  875  Cost: 0.2471\n",
      "Iteration:  876  Cost: 0.2470\n",
      "Iteration:  877  Cost: 0.2470\n",
      "Iteration:  878  Cost: 0.2469\n",
      "Iteration:  879  Cost: 0.2468\n",
      "Iteration:  880  Cost: 0.2468\n",
      "Iteration:  881  Cost: 0.2467\n",
      "Iteration:  882  Cost: 0.2466\n",
      "Iteration:  883  Cost: 0.2466\n",
      "Iteration:  884  Cost: 0.2465\n",
      "Iteration:  885  Cost: 0.2465\n",
      "Iteration:  886  Cost: 0.2464\n",
      "Iteration:  887  Cost: 0.2463\n",
      "Iteration:  888  Cost: 0.2463\n",
      "Iteration:  889  Cost: 0.2462\n",
      "Iteration:  890  Cost: 0.2461\n",
      "Iteration:  891  Cost: 0.2461\n",
      "Iteration:  892  Cost: 0.2460\n",
      "Iteration:  893  Cost: 0.2459\n",
      "Iteration:  894  Cost: 0.2459\n",
      "Iteration:  895  Cost: 0.2458\n",
      "Iteration:  896  Cost: 0.2458\n",
      "Iteration:  897  Cost: 0.2457\n",
      "Iteration:  898  Cost: 0.2456\n",
      "Iteration:  899  Cost: 0.2456\n",
      "Iteration:  900  Cost: 0.2455\n",
      "Iteration:  901  Cost: 0.2454\n",
      "Iteration:  902  Cost: 0.2454\n",
      "Iteration:  903  Cost: 0.2453\n",
      "Iteration:  904  Cost: 0.2453\n",
      "Iteration:  905  Cost: 0.2452\n",
      "Iteration:  906  Cost: 0.2451\n",
      "Iteration:  907  Cost: 0.2451\n",
      "Iteration:  908  Cost: 0.2450\n",
      "Iteration:  909  Cost: 0.2449\n",
      "Iteration:  910  Cost: 0.2449\n",
      "Iteration:  911  Cost: 0.2448\n",
      "Iteration:  912  Cost: 0.2448\n",
      "Iteration:  913  Cost: 0.2447\n",
      "Iteration:  914  Cost: 0.2446\n",
      "Iteration:  915  Cost: 0.2446\n",
      "Iteration:  916  Cost: 0.2445\n",
      "Iteration:  917  Cost: 0.2445\n",
      "Iteration:  918  Cost: 0.2444\n",
      "Iteration:  919  Cost: 0.2443\n",
      "Iteration:  920  Cost: 0.2443\n",
      "Iteration:  921  Cost: 0.2442\n",
      "Iteration:  922  Cost: 0.2442\n",
      "Iteration:  923  Cost: 0.2441\n",
      "Iteration:  924  Cost: 0.2440\n",
      "Iteration:  925  Cost: 0.2440\n",
      "Iteration:  926  Cost: 0.2439\n",
      "Iteration:  927  Cost: 0.2439\n",
      "Iteration:  928  Cost: 0.2438\n",
      "Iteration:  929  Cost: 0.2437\n",
      "Iteration:  930  Cost: 0.2437\n",
      "Iteration:  931  Cost: 0.2436\n",
      "Iteration:  932  Cost: 0.2436\n",
      "Iteration:  933  Cost: 0.2435\n",
      "Iteration:  934  Cost: 0.2434\n",
      "Iteration:  935  Cost: 0.2434\n",
      "Iteration:  936  Cost: 0.2433\n",
      "Iteration:  937  Cost: 0.2433\n",
      "Iteration:  938  Cost: 0.2432\n",
      "Iteration:  939  Cost: 0.2432\n",
      "Iteration:  940  Cost: 0.2431\n",
      "Iteration:  941  Cost: 0.2430\n",
      "Iteration:  942  Cost: 0.2430\n",
      "Iteration:  943  Cost: 0.2429\n",
      "Iteration:  944  Cost: 0.2429\n",
      "Iteration:  945  Cost: 0.2428\n",
      "Iteration:  946  Cost: 0.2428\n",
      "Iteration:  947  Cost: 0.2427\n",
      "Iteration:  948  Cost: 0.2426\n",
      "Iteration:  949  Cost: 0.2426\n",
      "Iteration:  950  Cost: 0.2425\n",
      "Iteration:  951  Cost: 0.2425\n",
      "Iteration:  952  Cost: 0.2424\n",
      "Iteration:  953  Cost: 0.2423\n",
      "Iteration:  954  Cost: 0.2423\n",
      "Iteration:  955  Cost: 0.2422\n",
      "Iteration:  956  Cost: 0.2422\n",
      "Iteration:  957  Cost: 0.2421\n",
      "Iteration:  958  Cost: 0.2421\n",
      "Iteration:  959  Cost: 0.2420\n",
      "Iteration:  960  Cost: 0.2420\n",
      "Iteration:  961  Cost: 0.2419\n",
      "Iteration:  962  Cost: 0.2418\n",
      "Iteration:  963  Cost: 0.2418\n",
      "Iteration:  964  Cost: 0.2417\n",
      "Iteration:  965  Cost: 0.2417\n",
      "Iteration:  966  Cost: 0.2416\n",
      "Iteration:  967  Cost: 0.2416\n",
      "Iteration:  968  Cost: 0.2415\n",
      "Iteration:  969  Cost: 0.2414\n",
      "Iteration:  970  Cost: 0.2414\n",
      "Iteration:  971  Cost: 0.2413\n",
      "Iteration:  972  Cost: 0.2413\n",
      "Iteration:  973  Cost: 0.2412\n",
      "Iteration:  974  Cost: 0.2412\n",
      "Iteration:  975  Cost: 0.2411\n",
      "Iteration:  976  Cost: 0.2411\n",
      "Iteration:  977  Cost: 0.2410\n",
      "Iteration:  978  Cost: 0.2410\n",
      "Iteration:  979  Cost: 0.2409\n",
      "Iteration:  980  Cost: 0.2408\n",
      "Iteration:  981  Cost: 0.2408\n",
      "Iteration:  982  Cost: 0.2407\n",
      "Iteration:  983  Cost: 0.2407\n",
      "Iteration:  984  Cost: 0.2406\n",
      "Iteration:  985  Cost: 0.2406\n",
      "Iteration:  986  Cost: 0.2405\n",
      "Iteration:  987  Cost: 0.2405\n",
      "Iteration:  988  Cost: 0.2404\n",
      "Iteration:  989  Cost: 0.2404\n",
      "Iteration:  990  Cost: 0.2403\n",
      "Iteration:  991  Cost: 0.2402\n",
      "Iteration:  992  Cost: 0.2402\n",
      "Iteration:  993  Cost: 0.2401\n",
      "Iteration:  994  Cost: 0.2401\n",
      "Iteration:  995  Cost: 0.2400\n",
      "Iteration:  996  Cost: 0.2400\n",
      "Iteration:  997  Cost: 0.2399\n",
      "Iteration:  998  Cost: 0.2399\n",
      "Iteration:  999  Cost: 0.2398\n",
      "Final cost: 0.2398\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta = np.zeros(X.shape[1])\n",
    "theta = gradientDescent(X_train, Y_train,theta,0.00001,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(x, theta):\n",
    "    return 1 if sigmoid(np.dot(x, theta)) >= 0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_regression(x, y, theta):\n",
    "    correct = 0\n",
    "    for i in range(len(x)):\n",
    "        if predict_class(x[i], theta) == y[i]:\n",
    "            correct += 1\n",
    "    return correct / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8938053097345132"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_logistic_regression(X_test, Y_test, theta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
