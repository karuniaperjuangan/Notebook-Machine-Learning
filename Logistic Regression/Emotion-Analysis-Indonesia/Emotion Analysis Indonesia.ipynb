{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Karunia\n",
      "[nltk_data]     Perjuangan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "idn_stopwords = list(stopwords.words('indonesian'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idn_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga','gue','gw', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', 'kau','ku','terus','trs',\n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'tp', 'tpi','bgt',\n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label(label1:str,label2:str) -> int:\n",
    "    if label1 == label2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "  text = re.sub(\"\\[USERNAME\\]\", \" \", text)\n",
    "  text = re.sub(\"\\[URL\\]\", \" \", text)\n",
    "  text = re.sub(\"\\[SENSITIVE-NO\\]\", \" \", text)\n",
    "  text = re.sub('  +', ' ', text)\n",
    "  return text\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "  text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "  # text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "  text = re.sub('^(\\@\\w+ ?)+',' ',text)\n",
    "  text = re.sub(r'\\@\\w+',' ',text) # Remove every username\n",
    "  text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "  text = re.sub('/', ' ', text)\n",
    "  # text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "  return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "  text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "  return text\n",
    "\n",
    "def remove_stopword(text):\n",
    "  text = ' '.join(['' if word in idn_stopwords else word for word in text.split(' ')])\n",
    "  text = re.sub('  +', ' ', text)\n",
    "  text = text.strip()\n",
    "  return text\n",
    "  \n",
    "\n",
    "def sastrawi_stemming(text):\n",
    "  factory = StemmerFactory()\n",
    "  stemmer = factory.create_stemmer()\n",
    "  text = stemmer.stem(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "  text = preprocess_tweet(text)\n",
    "  text = remove_unnecessary_char(text)\n",
    "  text = text.lower()\n",
    "  text = remove_nonaplhanumeric(text)\n",
    "  text = remove_stopword(text)\n",
    "  text = sastrawi_stemming(text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memberi indeks nilai 0 atau 1 pada setiap jenis label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "anger      500\n",
       "fear       500\n",
       "happy      500\n",
       "love       500\n",
       "sadness    500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_unbalanced = pd.read_csv('./emot_emotion-twitter/train_preprocess.csv')\n",
    "\n",
    "train_df = train_df_unbalanced.groupby('label').apply(lambda x: x.sample(500)).reset_index(drop=True)\n",
    "train_df.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tweet'] = train_df['tweet'].apply(preprocess)\n",
    "for emotion in ['happy','anger','fear','love','sadness']:\n",
    "    train_df[emotion] = train_df.apply(lambda row: check_label(row['label'],emotion),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(df):\n",
    "    freqs = {}\n",
    "    for row in df.itertuples():\n",
    "        for word in row.tweet.split(' '):\n",
    "            pair = (row.label,word)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs\n",
    "\n",
    "freqs = build_freqs(train_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('anger', 'salah'),\n",
       " ('anger', 'udah'),\n",
       " ('anger', 'orang'),\n",
       " ('anger', 'gitu'),\n",
       " ('anger', 'lu'),\n",
       " ('fear', 'rem'),\n",
       " ('fear', 'gimana'),\n",
       " ('fear', 'jalan'),\n",
       " ('fear', 'takut'),\n",
       " ('fear', 'orang'),\n",
       " ('fear', 'ngeri'),\n",
       " ('fear', 'pake'),\n",
       " ('fear', 'banget'),\n",
       " ('fear', 'gitu'),\n",
       " ('fear', 'udah'),\n",
       " ('fear', 'pas'),\n",
       " ('fear', 'sakit'),\n",
       " ('fear', 'rumah'),\n",
       " ('fear', 'temen'),\n",
       " ('fear', 'salah'),\n",
       " ('fear', 'sampe'),\n",
       " ('fear', 'hati'),\n",
       " ('fear', 'liat'),\n",
       " ('fear', 'w'),\n",
       " ('fear', 'gua'),\n",
       " ('happy', 'orang'),\n",
       " ('happy', 'happy'),\n",
       " ('happy', 'moga'),\n",
       " ('happy', 'bahagia'),\n",
       " ('happy', 'alhamdulillah'),\n",
       " ('happy', 'udah'),\n",
       " ('happy', 'banget'),\n",
       " ('happy', 'anak'),\n",
       " ('happy', 'suka'),\n",
       " ('happy', 'kasih'),\n",
       " ('happy', 'selamat'),\n",
       " ('love', 'udah'),\n",
       " ('love', 'gua'),\n",
       " ('love', 'jatuh'),\n",
       " ('love', 'cinta'),\n",
       " ('love', 'sayang'),\n",
       " ('love', 'suka'),\n",
       " ('love', 'love'),\n",
       " ('love', 'orang'),\n",
       " ('love', 'banget'),\n",
       " ('love', 'i'),\n",
       " ('love', 'you'),\n",
       " ('love', 'hati'),\n",
       " ('love', 'kasih'),\n",
       " ('love', 'senyum'),\n",
       " ('love', 'anak'),\n",
       " ('love', 'papa'),\n",
       " ('sadness', 'salah'),\n",
       " ('sadness', 'sakit'),\n",
       " ('sadness', 'orang'),\n",
       " ('sadness', 'udah'),\n",
       " ('sadness', 'cinta'),\n",
       " ('sadness', 'sayang'),\n",
       " ('sadness', 'hati'),\n",
       " ('sadness', 'jalan'),\n",
       " ('sadness', 'anak'),\n",
       " ('sadness', 'pas')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in freqs.keys() if freqs[key] >30 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Logistik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def gradient_descent(X,y,theta,alpha,iteration):\n",
    "    m = len(y)\n",
    "\n",
    "\n",
    "    for i in range(iteration):\n",
    "        h = sigmoid(X @ theta)\n",
    "        J = (-1/m) * (y.T @ np.log(h) + (1-y).T @ np.log(1-h))\n",
    "        #if (i+1)%500 == 0:\n",
    "        #    print(f\"Loss Iteration {i+1}: {float(J)}\")\n",
    "        theta = theta - alpha/m * X.T @ (h-y)\n",
    "    print(f\"Final Loss: {J}\")\n",
    "    print(theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    processed_tweet = preprocess(tweet)\n",
    "    words = processed_tweet.split(' ')\n",
    "\n",
    "    x = np.zeros((1,6)) # Bias + 5 emotions (Happy,Anger,Fear,Love,Sadness)\n",
    "    x[0,0] = 1\n",
    "\n",
    "    for word in words:\n",
    "        x[0,1] += freqs.get(('happy',word),0)\n",
    "        x[0,2] += freqs.get(('anger',word),0)\n",
    "        x[0,3] += freqs.get(('fear',word),0)\n",
    "        x[0,4] += freqs.get(('love',word),0)\n",
    "        x[0,5] += freqs.get(('sadness',word),0)\n",
    "    assert x.shape == (1,6)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 66., 34., 75., 89., 47.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tes\n",
    "extract_features(\"kamu jelek banget deh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(train_df),6))\n",
    "for i in range(len(train_df)):\n",
    "    X[i,:] = extract_features(train_df['tweet'][i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: [[0.45231885]]\n",
      "[[-3.03360230e-05]\n",
      " [ 1.96562408e-03]\n",
      " [-2.30020817e-03]\n",
      " [-4.02073332e-03]\n",
      " [-3.28024441e-03]\n",
      " [-1.90637626e-03]]\n",
      "Final Loss: [[0.43673554]]\n",
      "[[-2.96929028e-05]\n",
      " [-2.39247924e-03]\n",
      " [ 2.42719573e-03]\n",
      " [-3.38929593e-03]\n",
      " [-4.48728670e-03]\n",
      " [-1.74003053e-03]]\n",
      "Final Loss: [[0.38331063]]\n",
      "[[-6.28525873e-05]\n",
      " [-4.12806088e-03]\n",
      " [-3.64019281e-03]\n",
      " [ 5.71363199e-03]\n",
      " [-5.33884065e-03]\n",
      " [-3.56748712e-03]]\n",
      "Final Loss: [[0.34671125]]\n",
      "[[-7.06752269e-05]\n",
      " [-4.16589876e-03]\n",
      " [-4.76806607e-03]\n",
      " [-5.86872419e-03]\n",
      " [ 5.43801489e-03]\n",
      " [-4.04873258e-03]]\n",
      "Final Loss: [[0.49383138]]\n",
      "[[-3.82714487e-05]\n",
      " [-2.10429727e-03]\n",
      " [-1.82670907e-03]\n",
      " [-3.39857020e-03]\n",
      " [-2.77815290e-03]\n",
      " [ 1.34629703e-03]]\n"
     ]
    }
   ],
   "source": [
    "[theta_happy,theta_anger,theta_fear,theta_love,theta_sadness] = [np.zeros((6,1)) for i in range(5)]\n",
    "list_theta = [theta_happy,theta_anger,theta_fear,theta_love,theta_sadness]\n",
    "\n",
    "[Y_happy,Y_anger,Y_fear,Y_love,Y_sadness] = [train_df[emotion].values.reshape(-1,1) for emotion in ['happy','anger','fear','love','sadness']]\n",
    "list_Y = [Y_happy,Y_anger,Y_fear,Y_love,Y_sadness]\n",
    "\n",
    "len(list(zip(list_theta,list_Y)))\n",
    "\n",
    "[theta_happy,theta_anger,theta_fear,theta_love,theta_sadness] = [gradient_descent(X,Y,theta,1e-7,5000) for theta,Y in zip(list_theta,list_Y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.28525873e-05]\n",
      " [-4.12806088e-03]\n",
      " [-3.64019281e-03]\n",
      " [ 5.71363199e-03]\n",
      " [-5.33884065e-03]\n",
      " [-3.56748712e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(theta_fear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(tweet):\n",
    "    x = extract_features(tweet)\n",
    "    probs = [sigmoid(x @ theta) for theta in [theta_happy,theta_anger,theta_fear,theta_love,theta_sadness]]\n",
    "    softmax = np.exp(probs)/np.sum(np.exp(probs))\n",
    "    label = ['happy','anger','fear','love','sadness'][np.argmax(softmax)]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing in Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./emot_emotion-twitter/valid_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "def calculate_accuracy(df):\n",
    "    df['predicted_label'] = df['tweet'].apply(predict_emotion)\n",
    "    correct = 0\n",
    "    for index,row in df.iterrows():\n",
    "        if row['label'] == row['predicted_label']:\n",
    "            correct += 1\n",
    "    return correct/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5136363636363637"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_label</th>\n",
       "      <th>anger</th>\n",
       "      <th>fear</th>\n",
       "      <th>happy</th>\n",
       "      <th>love</th>\n",
       "      <th>sadness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.309091</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.145455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.169231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>0.049020</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.050505</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.373737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_label     anger      fear     happy      love   sadness\n",
       "label                                                            \n",
       "anger            0.463636  0.309091  0.027273  0.054545  0.145455\n",
       "fear             0.076923  0.692308  0.015385  0.046154  0.169231\n",
       "happy            0.049020  0.186275  0.382353  0.147059  0.235294\n",
       "love             0.000000  0.062500  0.031250  0.843750  0.062500\n",
       "sadness          0.121212  0.232323  0.050505  0.222222  0.373737"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(test_df['label'],test_df['predicted_label'],normalize='index')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
